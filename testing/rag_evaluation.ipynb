{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lisis de los diferentes modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de librer√≠as y modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisbarcap/.pyenv/versions/3.12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/luisbarcap/.pyenv/versions/3.12.2/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = \"baai_small\"\n",
    "llm_model = \"llama3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_loader = PyPDFDirectoryLoader(\"./data_testing\")\n",
    "documents = document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creaci√≥n de la base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name google/canine-c. Creating a new one with mean pooling.\n",
      "/Users/luisbarcap/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "üëâ Adding new documents: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisbarcap/.pyenv/versions/3.12.2/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(\n",
    "    persist_directory=\"./database_testing\", embedding_function=get_embedding_function(emb_model)\n",
    ")\n",
    "\n",
    "# Calculate Page IDs.\n",
    "chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "# Add or Update the documents.\n",
    "existing_items = db.get(include=[])  # IDs are always included by default\n",
    "existing_ids = set(existing_items[\"ids\"])\n",
    "print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "# Only add documents that don't exist in the DB.\n",
    "new_chunks = []\n",
    "for chunk in chunks_with_ids:\n",
    "    if chunk.metadata[\"id\"] not in existing_ids:\n",
    "        new_chunks.append(chunk)\n",
    "\n",
    "if len(new_chunks):\n",
    "    print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "    new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "    db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "    db.persist()\n",
    "else:\n",
    "    print(\"‚úÖ No new documents to add\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preguntas de inter√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulamos las preguntas y buscamos en la base de datos los chunks que nos ofrezcan mejor contexto para responderlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_en = [\n",
    "    \"In what year and city was Miguel de Cervantes born?\",\n",
    "    \"In what year and city was William Shakespeare born?\",\n",
    "    \"In what year and city was Joan Ramis i Ramis born?\",\n",
    "]\n",
    "\n",
    "questions_es = [\n",
    "    \"¬øEn qu√© a√±o y ciudad naci√≥ Miguel de Cervantes?\",\n",
    "    \"¬øEn qu√© a√±o y ciudad naci√≥ William Shakespeare?\",\n",
    "    \"¬øEn qu√© a√±o y ciudad naci√≥ Joan Ramis i Ramis?\",\n",
    "]\n",
    "\n",
    "questions_cat = [\n",
    "    \"En quin any i ciutat va n√©ixer Miguel de Cervantes?\",\n",
    "    \"En quin any i ciutat va n√©ixer William Shakespeare?\",\n",
    "    \"En quin any i ciutat va n√©ixer Joan Ramis i Ramis?\",\n",
    "]\n",
    "\n",
    "questions = questions_en + questions_es + questions_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "\n",
    "for question in questions:\n",
    "    # Get the top 5 most relevant documents\n",
    "    results = db.similarity_search_with_score(question, k=5)\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    # Append the context to the list of contexts\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos las respuestas esperadas para cada pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_en = [\n",
    "    \"Miguel de Cervantes was born in 1547 in Alcal√° de Henares.\",\n",
    "    \"William Shakespeare was born in 1564 in Stratford-upon-Avon.\",\n",
    "    \"Joan Ramis i Ramis was born in 1746 in Palma.\",\n",
    "]\n",
    "\n",
    "answers_es = [\n",
    "    \"Miguel de Cervantes naci√≥ en 1547 en Alcal√° de Henares.\",\n",
    "    \"William Shakespeare naci√≥ en 1564 en Stratford-upon-Avon.\",\n",
    "    \"Joan Ramis i Ramis naci√≥ en 1746 en Palma.\",\n",
    "]\n",
    "\n",
    "answers_cat = [\n",
    "    \"Miguel de Cervantes va n√©ixer el 1547 a Alcal√° de Henares.\",\n",
    "    \"William Shakespeare va n√©ixer el 1564 a Stratford-upon-Avon.\",\n",
    "    \"Joan Ramis i Ramis va n√©ixer el 1746 a Palma.\",\n",
    "]\n",
    "\n",
    "answers = answers_en + answers_es + answers_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente creamos el DataFrame para evaluar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': answers,\n",
    "    'contexts' : contexts,\n",
    "    'ground_truth': answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisbarcap/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name google/canine-c. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embedding_function(emb_model)\n",
    "llm = Ollama(model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset feature \"contexts\" should be of type Sequence[string], got <class 'datasets.features.features.Value'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_recall\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m df_score \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_score)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/ragas/evaluation.py:157\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    155\u001b[0m dataset \u001b[38;5;241m=\u001b[39m handle_deprecated_ground_truths(dataset)\n\u001b[1;32m    156\u001b[0m validate_evaluation_modes(dataset, metrics)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mvalidate_column_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, LangchainLLM):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/ragas/validation.py:56\u001b[0m, in \u001b[0;36mvalidate_column_dtypes\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column_names \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(ds\u001b[38;5;241m.\u001b[39mfeatures[column_names], Sequence)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mfeatures[column_names]\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     ):\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset feature \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m should be of type\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Sequence[string], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ds\u001b[38;5;241m.\u001b[39mfeatures[column_names])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset feature \"contexts\" should be of type Sequence[string], got <class 'datasets.features.features.Value'>"
     ]
    }
   ],
   "source": [
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "df_score = score.to_pandas()\n",
    "print(df_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
