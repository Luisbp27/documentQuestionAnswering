{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.25s/it]\n",
      "/Users/luisbarcap/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "embedding nodes:  33%|███▎      | 18/54 [03:36<13:29, 22.49s/it]"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_community.llms import Ollama\n",
    "from module import get_embedding_function\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# load documents\n",
    "loader = DirectoryLoader(\"./data_testing\", show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = Ollama(model=\"llama3\")\n",
    "critic_llm = Ollama(model=\"mistral\")\n",
    "embeddings = get_embedding_function(\"baai_small\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, 10,distributions)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = test_df['question'].values.tolist()\n",
    "test_answers = [[item] for item in test_df['answer'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings import HuggingFaceInferenceAPIEmbedding\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def build_query_engine(llm):\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=ServiceContext.from_defaults(chunk_size=512, llm=llm),\n",
    "        embed_model=HuggingFaceInferenceAPIEmbedding,\n",
    "    )\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "    return query_engine\n",
    "\n",
    "# Function to evaluate as Llama index does not support async evaluation for HFInference API\n",
    "def generate_responses(query_engine, test_questions, test_answers):\n",
    "  responses = [query_engine.query(q) for q in test_questions]\n",
    "\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  for r in responses:\n",
    "    answers.append(r.response)\n",
    "    contexts.append([c.node.get_content() for c in r.source_nodes])\n",
    "  dataset_dict = {\n",
    "        \"question\": test_questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "  }\n",
    "  if test_answers is not None:\n",
    "    dataset_dict[\"ground_truth\"] = test_answers\n",
    "  ds = Dataset.from_dict(dataset_dict)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HFTOKEN from environment variable\n",
    "import os\n",
    "HFTOKEN = os.getenv(\"HFTOKEN\")\n",
    "\n",
    "# Use zephyr model using HFInference API\n",
    "zephyr_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    token=HFTOKEN\n",
    ")\n",
    "query_engine1 = build_query_engine(zephyr_llm)\n",
    "result_ds = generate_responses(query_engine1, test_questions, test_answers)\n",
    "result_zephyr = evaluate(\n",
    "    result_ds,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "result_zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"tiiuae/falcon-7b-instruct\",\n",
    "    token=HFTOKEN\n",
    ")\n",
    "query_engine2 = build_query_engine(falcon_llm)\n",
    "result_ds_falcon = generate_responses(query_engine2, test_questions, test_answers)\n",
    "result = evaluate(\n",
    "    result_ds_falcon,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
