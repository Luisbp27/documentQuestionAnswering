{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DirectoryLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_community.llms import Ollama\n",
    "from module import get_embedding_function\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# load documents\n",
    "loader = DirectoryLoader(\"./data_testing\", show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = Ollama(model=\"llama3\")\n",
    "critic_llm = Ollama(model=\"mistral\")\n",
    "embeddings = get_embedding_function(\"baai_small\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_llama_index_docs(documents, 100,distributions)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = test_df['question'].values.tolist()\n",
    "test_answers = [[item] for item in test_df['answer'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings import HuggingFaceInferenceAPIEmbedding\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def build_query_engine(llm):\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=ServiceContext.from_defaults(chunk_size=512, llm=llm),\n",
    "        embed_model=HuggingFaceInferenceAPIEmbedding,\n",
    "    )\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "    return query_engine\n",
    "\n",
    "# Function to evaluate as Llama index does not support async evaluation for HFInference API\n",
    "def generate_responses(query_engine, test_questions, test_answers):\n",
    "  responses = [query_engine.query(q) for q in test_questions]\n",
    "\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  for r in responses:\n",
    "    answers.append(r.response)\n",
    "    contexts.append([c.node.get_content() for c in r.source_nodes])\n",
    "  dataset_dict = {\n",
    "        \"question\": test_questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "  }\n",
    "  if test_answers is not None:\n",
    "    dataset_dict[\"ground_truth\"] = test_answers\n",
    "  ds = Dataset.from_dict(dataset_dict)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HFTOKEN from environment variable\n",
    "import os\n",
    "HFTOKEN = os.getenv(\"HFTOKEN\")\n",
    "\n",
    "# Use zephyr model using HFInference API\n",
    "zephyr_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    token=HFTOKEN\n",
    ")\n",
    "query_engine1 = build_query_engine(zephyr_llm)\n",
    "result_ds = generate_responses(query_engine1, test_questions, test_answers)\n",
    "result_zephyr = evaluate(\n",
    "    result_ds,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "result_zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"tiiuae/falcon-7b-instruct\",\n",
    "    token=HFTOKEN\n",
    ")\n",
    "query_engine2 = build_query_engine(falcon_llm)\n",
    "result_ds_falcon = generate_responses(query_engine2, test_questions, test_answers)\n",
    "result = evaluate(\n",
    "    result_ds_falcon,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
